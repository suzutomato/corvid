# -*- coding: utf-8 -*-

import argparse
from copy import deepcopy
import os
from pathlib import Path
import pickle
import sys

from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings


def generate_settings(site_name, base_dir, base_settings, start_time):

    settings = deepcopy(base_settings)

    site_dir = base_dir / site_name
    latest_dir = site_dir / 'LATEST'
    daily_dir = site_dir / start_time

    if not latest_dir.exists():
        latest_dir.mkdir(parents=True)
    if not daily_dir.exists():
        daily_dir.mkdir()

    target_forums_path = base_dir / 'corvid_config' / 'target_forums.pickle'
    if not target_forums_path.exists():
        raise FileNotFoundError(
            f'`target_forums.pickle` doesn\'t exist in {base_dir}'
        )
    with target_forums_path.open('rb') as rh:
        target_formus = pickle.load(rh)

    settings['TARGET_FORUMS'] = target_formus

    settings['IMAGES_STORE'] = str(base_dir/'images')
    settings['LATEST_DIR_POINTER'] = str(latest_dir/'latest.txt')
    settings['DAILY_DIR'] = str(daily_dir)
    settings['DATA_DIR'] = str(daily_dir/'data')

    settings['FORUM_DIR_TEMPLATE'] = '{forum_id}'
    settings['FORUM_METADATA_TEMPLATE'] = '{forum_id}_metadata.json'
    settings['TOPIC_DIR_TEMPLATE'] = '{forum_id}/{topic_id}'
    settings['TOPIC_METADATA_TEMPLATE'] = '{topic_id}_metadata.json'
    settings['TOPIC_CONTENTS_TEMPLATE'] = '{topic_id}_contents.csv'

    settings['LOG_LEVEL'] = 'WARNING'
    settings['LOG_FILE'] = str(daily_dir/'log.txt')

    return settings


if __name__ == '__main__':
    src_path = str(Path(__file__).resolve().parents[1])
    sys.path.insert(0, src_path)

    # Config ArgumentParser
    parser = argparse.ArgumentParser(description='Parse Forums')
    parser.add_argument('-d', default='.', type=str, required=False,
                        dest='base_dir',
                        help='Base directory for files generated by spiders')
    parser.add_argument('-t', type=str, required=True, dest='start_time',
                        help='When the task is started, used for folder names')
    parser.add_argument('-s', type=str, required=True, dest='site',
                        help='Spider name')

    # Parse arguments
    args = parser.parse_args()
    base_dir = Path(args.base_dir)
    start_time = args.start_time
    site = args.site

    # Set BASE_DIR environment variable for utils.urlutil
    os.environ['SCRAPER_CONFIG_DIR'] = str(base_dir/'corvid_config')

    from forum_scraper.spiders.a2ch import A2chSpider
    from forum_scraper.spiders.a5ch import A5chSpider

    site_spider_mapping = {
        '2ch': A2chSpider,
        '5ch': A5chSpider
    }

    # Retrieve the settings from `settings.py` file
    os.environ['SCRAPY_SETTINGS_MODULE'] = 'forum_scraper.settings'
    base_settings = get_project_settings()

    settings = generate_settings(site, base_dir, base_settings, start_time)
    process = CrawlerProcess(settings)
    process.crawl(site_spider_mapping[site])
    process.start()
